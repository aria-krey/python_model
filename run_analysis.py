"""
–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ NSL-KDD
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from src.data_loader import NSLKDDDataLoader
from src.preprocessor import DataPreprocessor
from src.models import ModelTrainer
from sklearn.metrics import confusion_matrix
import warnings
from imblearn.over_sampling import SMOTE
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
plt.rcParams['figure.figsize'] = (12, 8)
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ–≥–æ –∫–æ–¥–∞"""
    print("=" * 70)
    print("–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–ï–¢–ï–í–û–ì–û –¢–†–ê–§–ò–ö–ê –ù–ê –û–°–ù–û–í–ï NSL-KDD")
    print("=" * 70)

    # 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 
    print("\n –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)

    loader = NSLKDDDataLoader(data_path='data')

    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    train_data = loader.load_data('train', use_20percent=False)
    test_data = loader.load_data('test')

    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    print("\n –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
    loader.analyze_dataset(train_data)

    # 2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
    print("\n\n –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)

    # –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (True - –±–∏–Ω–∞—Ä–Ω–∞—è, False - –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è)
    BINARY_CLASSIFICATION = False  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ True –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π

    print(f"\n–¢–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {'–ë–∏–Ω–∞—Ä–Ω–∞—è (–∞—Ç–∞–∫–∞/–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π)' if BINARY_CLASSIFICATION else '–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è (5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π)'}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    X_train, y_train, target_name = loader.prepare_target(
        train_data,
        binary=BINARY_CLASSIFICATION,
        filter_unknown=True
    )

    X_test, y_test, _ = loader.prepare_target(
        test_data,
        binary=BINARY_CLASSIFICATION,
        filter_unknown=True
    )

    print(f"\n–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:")
    train_dist = pd.Series(y_train).value_counts()
    for class_name, count in train_dist.items():
        percentage = (count / len(y_train)) * 100
        print(f"  {class_name}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")

    print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
    test_dist = pd.Series(y_test).value_counts()
    for class_name, count in test_dist.items():
        percentage = (count / len(y_test)) * 100
        print(f"  {class_name}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    train_classes = set(y_train.unique())
    test_classes = set(y_test.unique())

    print(f"\n –ö–ª–∞—Å—Å—ã –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {sorted(train_classes)}")
    print(f" –ö–ª–∞—Å—Å—ã –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {sorted(test_classes)}")

    # 3. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê 
    print("\n\n –®–ê–ì 3: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    categorical_features, numerical_features = loader.get_feature_types(X_train)

    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(categorical_features)}): {categorical_features}")
    print(f"–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(numerical_features)}): –ø–µ—Ä–≤—ã–µ 5 - {numerical_features[:5]}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    preprocessor = DataPreprocessor(categorical_features, numerical_features)

    print("\n–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")

    # –û–±—É—á–∞–µ–º –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    X_train_processed, y_train_encoded = preprocessor.fit_transform(X_train, y_train)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    X_test_processed, y_test_encoded = preprocessor.transform(X_test, y_test)

    print("\n –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (SMOTE)...")

    smote = SMOTE(
        sampling_strategy='not majority',
        random_state=42,
        k_neighbors=5
    )

    X_train_balanced, y_train_balanced = smote.fit_resample(
        X_train_processed,
        y_train_encoded
    )

    print("–†–∞–∑–º–µ—Ä—ã –ø–æ—Å–ª–µ SMOTE:")
    print("X_train:", X_train_balanced.shape)
    print("y_train:", y_train_balanced.shape)

    print(f" –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"  X_train: {X_train_processed.shape}")
    print(f"  X_test: {X_test_processed.shape}")

    # –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
    print(f"\n –ö–ª–∞—Å—Å—ã –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:")
    class_names = preprocessor.get_class_names()
    print(f"  –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {class_names}")

    # 4. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –° –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ï–ô 
    print("\n\n –®–ê–ì 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
    print("-" * 40)

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π
    model_params = {
        'Decision Tree': {
            'max_depth': 10,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
            'min_samples_split': 20,
            'min_samples_leaf': 10,
            'max_features': 'sqrt'
        },
        'Gradient Boosting': {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'min_samples_split': 20,
            'min_samples_leaf': 10,
            'subsample': 0.8,  # –°–ª—É—á–∞–π–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
            'max_features': 'sqrt'
        },
        'KNN': {
        'n_neighbors': 50,
        'weights': 'distance',
        'metric': 'minkowski',
        'p': 2
        },
        'Random Forest': {
        'n_estimators': 200,
        'max_depth': 12,
        'min_samples_split': 30,
        'min_samples_leaf': 15,
        'max_features': 'sqrt',
        'bootstrap': True,
        'class_weight': 'balanced',
        'n_jobs': -1
        }
    }

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    trainer = ModelTrainer(random_state=42)
    trainer.train_models(
        X_train_balanced, y_train_balanced,   # ‚Üê SMOTE
        X_test_processed, y_test_encoded,     # ‚Üê –ë–ï–ó SMOTE
        model_params=model_params
    )

    
    # 5. –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    print("\n\n –®–ê–ì 5: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("-" * 40)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    comparison_df = trainer.compare_models()
    print("\n –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
    print(comparison_df.to_string(index=False))

    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    print("\n –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:")
    for _, row in comparison_df.iterrows():
        train_val = row['Train CV (F1)']
        test_val = row['Test F1']

        diff = abs(train_val - test_val)

        if diff > 0.07:
            print(f"  {row['Model']}: –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞: {diff:.4f})")
        else:
            print(f"  {row['Model']}: –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º (—Ä–∞–∑–Ω–∏—Ü–∞: {diff:.4f})")


    # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
    best_model_name, best_result = trainer.get_best_model()
    print(f"\n –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}") 
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_result['test_accuracy']:.4f}")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    y_pred_best = best_result['predictions']

    print(f"\n –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç ({best_model_name}):")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º inverse_transform –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫
    y_test_original = preprocessor.inverse_transform(y_test_encoded)
    y_pred_original = preprocessor.inverse_transform(y_pred_best)

    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    report_dict = {}
    for class_name in class_names:
        if class_name in y_test_original:
            mask_true = y_test_original == class_name
            mask_pred = y_pred_original == class_name

            true_positives = np.sum((y_test_original == class_name) & (y_pred_original == class_name))
            false_positives = np.sum((y_test_original != class_name) & (y_pred_original == class_name))
            false_negatives = np.sum((y_test_original == class_name) & (y_pred_original != class_name))

            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            support = np.sum(mask_true)

            report_dict[class_name] = {
                'precision': precision,
                'recall': recall,
                'f1-score': f1,
                'support': support
            }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    precisions = [v['precision'] for v in report_dict.values()]
    recalls = [v['recall'] for v in report_dict.values()]
    f1_scores = [v['f1-score'] for v in report_dict.values()]
    supports = [v['support'] for v in report_dict.values()]

    report_dict['macro avg'] = {
        'precision': np.mean(precisions),
        'recall': np.mean(recalls),
        'f1-score': np.mean(f1_scores),
        'support': np.sum(supports)
    }

    report_dict['weighted avg'] = {
        'precision': np.average(precisions, weights=supports),
        'recall': np.average(recalls, weights=supports),
        'f1-score': np.average(f1_scores, weights=supports),
        'support': np.sum(supports)
    }

    report_df = pd.DataFrame(report_dict).transpose()
    print(report_df.round(3))

    # 6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø 
    print("\n\n –®–ê–ì 6: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("-" * 40)

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ results –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs('results', exist_ok=True)

    try:
        # 1. –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        comparison_df['Test F1'] = pd.to_numeric(comparison_df['Test F1'], errors='coerce')
        if 'Train CV (F1)' in comparison_df.columns:
            comparison_df['Train CV (F1)'] = pd.to_numeric(comparison_df['Train CV (F1)'], errors='coerce')
            train_col = 'Train CV (F1)'
        else:
            train_col = None
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
        comparison_df = comparison_df.fillna(0)
        
        plt.figure(figsize=(12, 8))
        
        models = comparison_df['Model']
        test_acc = comparison_df['Test Accuracy']
        
        x = np.arange(len(models))
        width = 0.6
        
        if train_col and train_col in comparison_df.columns:
            train_acc = comparison_df[train_col]
            width = 0.35
            
            plt.bar(x - width/2, train_acc, width, label='Train CV', alpha=0.8, color='steelblue')
            plt.bar(x + width/2, test_acc, width, label='Test', alpha=0.8, color='lightcoral')
            plt.legend()
        else:
            plt.bar(x, test_acc, width, alpha=0.8, color='lightcoral')
        
        plt.xlabel('–ú–æ–¥–µ–ª–∏', fontsize=12)
        plt.ylabel('F1 macro', fontsize=12)
        plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold')
        plt.xticks(x, models, rotation=45, ha='right')
        plt.grid(True, alpha=0.3, axis='y')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π - –ë–ï–ó–û–ü–ê–°–ù–´–ô –°–ü–û–°–û–ë
        for i, val in enumerate(test_acc):
            try:
                val_float = float(val)
                if val_float > 0:
                    plt.text(i if train_col is None else i + width/2, 
                            val_float + 0.01, f'{val_float:.3f}', 
                            ha='center', va='bottom', fontsize=9)
            except (ValueError, TypeError):
                continue
        
        if train_col and train_col in comparison_df.columns:
            for i, val in enumerate(train_acc):
                try:
                    val_float = float(val)
                    if val_float > 0:
                        plt.text(i - width/2, val_float + 0.01, f'{val_float:.3f}', 
                                ha='center', va='bottom', fontsize=9)
                except (ValueError, TypeError):
                    continue
        
        plt.tight_layout()
        plt.savefig('results/model_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(" –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: results/model_comparison.png")
        
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")

    try:
        # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ 
        print("\n2. –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
        
        plt.figure(figsize=(10, 6))
        
        if not train_dist.empty and len(train_dist) > 0:
            colors = plt.cm.Set3(np.linspace(0, 1, len(train_dist)))
            train_dist.plot(kind='bar', color=colors)
            
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ', fontsize=14, fontweight='bold')
            plt.xlabel('–ö–ª–∞—Å—Å', fontsize=12)
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π', fontsize=12)
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3, axis='y')
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
            for i, v in enumerate(train_dist.values):
                if not pd.isna(v):
                    plt.text(i, v + max(train_dist.values)*0.01, str(int(v)),
                            ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            plt.savefig('results/class_distribution.png', dpi=300, bbox_inches='tight')
            plt.show()
            print(" –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: results/class_distribution.png")
        else:
            print(" –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤")
            
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}")

    try:
        # 3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ 
        print("\n3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
        
        if y_pred_best is not None and len(y_pred_best) > 0:
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
            cm = confusion_matrix(y_test_encoded, y_pred_best)
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=class_names,
                       yticklabels=class_names)
            plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: {best_model_name}', fontsize=14, fontweight='bold')
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
            plt.tight_layout()
            plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')
            plt.show()
            print(f" –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: results/confusion_matrix.png")
        else:
            print(" –ù–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏")
            
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫: {e}")

    # 7. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó 
    print("\n\nüîç –®–ê–ì 7: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    print("-" * 40)

    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    try:
        if best_result['model'] is not None and hasattr(best_result['model'], 'feature_importances_'):
            print("\n –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")

            feature_names = preprocessor.get_feature_names()
            importances = best_result['model'].feature_importances_

            # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values('Importance', ascending=False)

            print("\n–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            print(feature_importance_df.head(10).to_string(index=False))

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            plt.figure(figsize=(12, 8))
            top_features = feature_importance_df.head(15)

            plt.barh(range(len(top_features)), top_features['Importance'])
            plt.yticks(range(len(top_features)), top_features['Feature'], fontsize=9)
            plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞', fontsize=12)
            plt.title('–¢–æ–ø-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.savefig('results/feature_importance.png', dpi=300, bbox_inches='tight')
            plt.show()
            print(" –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        else:
            print(" –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances_")
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    try:
        print("\nüìä –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        error_analysis = pd.DataFrame({
            'True': y_test_original,
            'Predicted': y_pred_original,
            'Correct': y_test_original == y_pred_original
        })

        error_by_class = error_analysis.groupby('True')['Correct'].agg(['mean', 'count'])
        error_by_class['error_rate'] = (1 - error_by_class['mean']) * 100
        error_by_class['error_count'] = error_by_class['count'] * (1 - error_by_class['mean'])

        print(error_by_class[['error_rate', 'error_count']].round(2))
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—à–∏–±–æ–∫: {e}")

    # 8. –ò–¢–û–ì–ò 
    print("\n\n" + "=" * 70)
    print(" –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 70)

    print(f"\n –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(train_data) + len(test_data)}")
    print(f"  ‚Ä¢ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π")
    print(f"  ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(y_test_encoded)} –∑–∞–ø–∏—Å–µ–π")
    print(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
    print(f"  ‚Ä¢ –ö–ª–∞—Å—Å—ã: {', '.join(class_names)}")

    print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"  ‚Ä¢ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"  ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_result['test_accuracy']:.4f}")
    
    if best_result.get('cv_mean') is not None:
        diff = abs(best_result['cv_mean'] - best_result['test_accuracy'])
        print(f"  ‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ CV/Test: {diff:.4f}")
        if diff > 0.05:
            print(f"  ‚ö† –í–Ω–∏–º–∞–Ω–∏–µ: –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞ > 0.05)")
        else:
            print(f"  ‚úì –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º")

    print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for class_name in class_names:
        count_train = (y_train == class_name).sum()
        count_test = (y_test_original == class_name).sum()
        percentage_train = (count_train / len(y_train)) * 100 if len(y_train) > 0 else 0
        percentage_test = (count_test / len(y_test_original)) * 100 if len(y_test_original) > 0 else 0
        print(f"  ‚Ä¢ {class_name}: –æ–±—É—á–∞—é—â–∏—Ö={count_train} ({percentage_train:.1f}%), —Ç–µ—Å—Ç–æ–≤—ã—Ö={count_test} ({percentage_test:.1f}%)")

    print(f"\n –í—ã–≤–æ–¥—ã:")
    print("  1. –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞")
    print("  2. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    print("  3. –ù–∞–∏–±–æ–ª—å—à–∏–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å —Ä–µ–¥–∫–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ (R2L, U2R)")
    print("  4. –í–∞–∂–Ω—ã–º–∏ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")

    print(f"\n –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:")
    print("  1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print("  2. –ü—Ä–∏–º–µ–Ω—è—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤ (SMOTE, ADASYN)")
    print("  3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤—å–µ–≤)")
    print("  4. –î–æ–±–∞–≤–∏—Ç—å dropout –≤ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã")
    print("  5. –£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        comparison_df.to_csv('results/model_comparison.csv', index=False)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report_df.to_csv('results/detailed_report.csv')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        pd.Series(y_train).value_counts().to_csv('results/class_distribution.csv')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open('results/config.txt', 'w', encoding='utf-8') as f:
            f.write(f"–î–∞—Ç–∞—Å–µ—Ç: NSL-KDD\n")
            f.write(f"–¢–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {'–ë–∏–Ω–∞—Ä–Ω–∞—è' if BINARY_CLASSIFICATION else '–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è'}\n")
            f.write(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}\n")
            f.write(f"–¢–æ—á–Ω–æ—Å—Ç—å: {best_result['test_accuracy']:.4f}\n")
            f.write(f"–ö–ª–∞—Å—Å—ã: {', '.join(class_names)}\n")
        
        print(" –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")
    except Exception as e:
        print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤: {e}")

    print("\n" + "=" * 70)
    print(" –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    print("=" * 70)

if __name__ == "__main__":
    main()
